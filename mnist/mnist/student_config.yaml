model:
  image_size: 28
  num_classes: 10
  n_hidden: 8 #32 #128
  n0: 1
  n1: 8 #32
  n2: 8 #64

optimizer:
  type: "adam"
  learning_rate: 0.001
  weight_decay: 0.0001

trainer:
  max_epochs: 5
  accelerator: "mps"
  #log_every_n_steps: 10

dataset:
  batch_size: 64
  num_workers: 2
  train_split: 0.8
  val_split: 0.2

distillation:
  temperature: 2.0
  alpha: 0.7